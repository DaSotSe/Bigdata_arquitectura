{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UYnEKbCg8DHv"
   },
   "source": [
    "# ¿Qué es el ***Crawling***?\n",
    "\n",
    "Es un proceso por el cual un robot (o spider web) navega sistemáticamente a través de internet explorando webs y siguiendo links con el propósito de recoger el contenido web en otro sistema. Todos los buscadores utilizan estos bots, llamados crawlers, para indexar las diferentes webs en sus buscadores.\n",
    "\n",
    "En definitiva, se trata de un proceso de rastreo web, donde se captura información general y estructura de la misma.\n",
    "\n",
    "## Y... ¿para qué sirve?\n",
    "\n",
    "El Crawling sirve para repasar todo el contenido y enlaces de una web con el fin de indexarlo todo en los buscadores correspondientes. Es decir, sirve para que cada actualización o nuevo contenido que se publique en una página acabe apareciendo en Google y demás motores.\n",
    "\n",
    "Además de para indexar el contenido en los buscadores, sirve para crear una copia de partes de una web y poder procesar esos datos obtenidos.\n",
    "\n",
    "![crawler](https://dc722jrlp2zu8.cloudfront.net/media/uploads/2019/06/17/p4x01.png)\n",
    "\n",
    "### Hagamos algunas pruebas. Hora de crawlear!\n",
    "\n",
    "Primero instalaremos una librería imprescindible:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "j6QDO36t-f9U"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: scrapy in /Users/davidsoteloseguin/Library/Mobile Documents/com~apple~CloudDocs/Personal/Formacion /Bootcamp/Bootcamp KC/Big-Data-Architecture/.venv/lib/python3.13/site-packages (2.12.0)\n",
      "Requirement already satisfied: Twisted>=21.7.0 in /Users/davidsoteloseguin/Library/Mobile Documents/com~apple~CloudDocs/Personal/Formacion /Bootcamp/Bootcamp KC/Big-Data-Architecture/.venv/lib/python3.13/site-packages (from scrapy) (24.11.0)\n",
      "Requirement already satisfied: cryptography>=37.0.0 in /Users/davidsoteloseguin/Library/Mobile Documents/com~apple~CloudDocs/Personal/Formacion /Bootcamp/Bootcamp KC/Big-Data-Architecture/.venv/lib/python3.13/site-packages (from scrapy) (44.0.2)\n",
      "Requirement already satisfied: cssselect>=0.9.1 in /Users/davidsoteloseguin/Library/Mobile Documents/com~apple~CloudDocs/Personal/Formacion /Bootcamp/Bootcamp KC/Big-Data-Architecture/.venv/lib/python3.13/site-packages (from scrapy) (1.2.0)\n",
      "Requirement already satisfied: itemloaders>=1.0.1 in /Users/davidsoteloseguin/Library/Mobile Documents/com~apple~CloudDocs/Personal/Formacion /Bootcamp/Bootcamp KC/Big-Data-Architecture/.venv/lib/python3.13/site-packages (from scrapy) (1.3.2)\n",
      "Requirement already satisfied: parsel>=1.5.0 in /Users/davidsoteloseguin/Library/Mobile Documents/com~apple~CloudDocs/Personal/Formacion /Bootcamp/Bootcamp KC/Big-Data-Architecture/.venv/lib/python3.13/site-packages (from scrapy) (1.10.0)\n",
      "Requirement already satisfied: pyOpenSSL>=22.0.0 in /Users/davidsoteloseguin/Library/Mobile Documents/com~apple~CloudDocs/Personal/Formacion /Bootcamp/Bootcamp KC/Big-Data-Architecture/.venv/lib/python3.13/site-packages (from scrapy) (25.0.0)\n",
      "Requirement already satisfied: queuelib>=1.4.2 in /Users/davidsoteloseguin/Library/Mobile Documents/com~apple~CloudDocs/Personal/Formacion /Bootcamp/Bootcamp KC/Big-Data-Architecture/.venv/lib/python3.13/site-packages (from scrapy) (1.7.0)\n",
      "Requirement already satisfied: service-identity>=18.1.0 in /Users/davidsoteloseguin/Library/Mobile Documents/com~apple~CloudDocs/Personal/Formacion /Bootcamp/Bootcamp KC/Big-Data-Architecture/.venv/lib/python3.13/site-packages (from scrapy) (24.2.0)\n",
      "Requirement already satisfied: w3lib>=1.17.0 in /Users/davidsoteloseguin/Library/Mobile Documents/com~apple~CloudDocs/Personal/Formacion /Bootcamp/Bootcamp KC/Big-Data-Architecture/.venv/lib/python3.13/site-packages (from scrapy) (2.3.1)\n",
      "Requirement already satisfied: zope.interface>=5.1.0 in /Users/davidsoteloseguin/Library/Mobile Documents/com~apple~CloudDocs/Personal/Formacion /Bootcamp/Bootcamp KC/Big-Data-Architecture/.venv/lib/python3.13/site-packages (from scrapy) (7.2)\n",
      "Requirement already satisfied: protego>=0.1.15 in /Users/davidsoteloseguin/Library/Mobile Documents/com~apple~CloudDocs/Personal/Formacion /Bootcamp/Bootcamp KC/Big-Data-Architecture/.venv/lib/python3.13/site-packages (from scrapy) (0.4.0)\n",
      "Requirement already satisfied: itemadapter>=0.1.0 in /Users/davidsoteloseguin/Library/Mobile Documents/com~apple~CloudDocs/Personal/Formacion /Bootcamp/Bootcamp KC/Big-Data-Architecture/.venv/lib/python3.13/site-packages (from scrapy) (0.11.0)\n",
      "Requirement already satisfied: packaging in /Users/davidsoteloseguin/Library/Mobile Documents/com~apple~CloudDocs/Personal/Formacion /Bootcamp/Bootcamp KC/Big-Data-Architecture/.venv/lib/python3.13/site-packages (from scrapy) (24.2)\n",
      "Requirement already satisfied: tldextract in /Users/davidsoteloseguin/Library/Mobile Documents/com~apple~CloudDocs/Personal/Formacion /Bootcamp/Bootcamp KC/Big-Data-Architecture/.venv/lib/python3.13/site-packages (from scrapy) (5.1.3)\n",
      "Requirement already satisfied: lxml>=4.6.0 in /Users/davidsoteloseguin/Library/Mobile Documents/com~apple~CloudDocs/Personal/Formacion /Bootcamp/Bootcamp KC/Big-Data-Architecture/.venv/lib/python3.13/site-packages (from scrapy) (5.3.1)\n",
      "Requirement already satisfied: defusedxml>=0.7.1 in /Users/davidsoteloseguin/Library/Mobile Documents/com~apple~CloudDocs/Personal/Formacion /Bootcamp/Bootcamp KC/Big-Data-Architecture/.venv/lib/python3.13/site-packages (from scrapy) (0.7.1)\n",
      "Requirement already satisfied: PyDispatcher>=2.0.5 in /Users/davidsoteloseguin/Library/Mobile Documents/com~apple~CloudDocs/Personal/Formacion /Bootcamp/Bootcamp KC/Big-Data-Architecture/.venv/lib/python3.13/site-packages (from scrapy) (2.0.7)\n",
      "Requirement already satisfied: cffi>=1.12 in /Users/davidsoteloseguin/Library/Mobile Documents/com~apple~CloudDocs/Personal/Formacion /Bootcamp/Bootcamp KC/Big-Data-Architecture/.venv/lib/python3.13/site-packages (from cryptography>=37.0.0->scrapy) (1.17.1)\n",
      "Requirement already satisfied: jmespath>=0.9.5 in /Users/davidsoteloseguin/Library/Mobile Documents/com~apple~CloudDocs/Personal/Formacion /Bootcamp/Bootcamp KC/Big-Data-Architecture/.venv/lib/python3.13/site-packages (from itemloaders>=1.0.1->scrapy) (1.0.1)\n",
      "Requirement already satisfied: attrs>=19.1.0 in /Users/davidsoteloseguin/Library/Mobile Documents/com~apple~CloudDocs/Personal/Formacion /Bootcamp/Bootcamp KC/Big-Data-Architecture/.venv/lib/python3.13/site-packages (from service-identity>=18.1.0->scrapy) (25.1.0)\n",
      "Requirement already satisfied: pyasn1 in /Users/davidsoteloseguin/Library/Mobile Documents/com~apple~CloudDocs/Personal/Formacion /Bootcamp/Bootcamp KC/Big-Data-Architecture/.venv/lib/python3.13/site-packages (from service-identity>=18.1.0->scrapy) (0.6.1)\n",
      "Requirement already satisfied: pyasn1-modules in /Users/davidsoteloseguin/Library/Mobile Documents/com~apple~CloudDocs/Personal/Formacion /Bootcamp/Bootcamp KC/Big-Data-Architecture/.venv/lib/python3.13/site-packages (from service-identity>=18.1.0->scrapy) (0.4.1)\n",
      "Requirement already satisfied: automat>=24.8.0 in /Users/davidsoteloseguin/Library/Mobile Documents/com~apple~CloudDocs/Personal/Formacion /Bootcamp/Bootcamp KC/Big-Data-Architecture/.venv/lib/python3.13/site-packages (from Twisted>=21.7.0->scrapy) (24.8.1)\n",
      "Requirement already satisfied: constantly>=15.1 in /Users/davidsoteloseguin/Library/Mobile Documents/com~apple~CloudDocs/Personal/Formacion /Bootcamp/Bootcamp KC/Big-Data-Architecture/.venv/lib/python3.13/site-packages (from Twisted>=21.7.0->scrapy) (23.10.4)\n",
      "Requirement already satisfied: hyperlink>=17.1.1 in /Users/davidsoteloseguin/Library/Mobile Documents/com~apple~CloudDocs/Personal/Formacion /Bootcamp/Bootcamp KC/Big-Data-Architecture/.venv/lib/python3.13/site-packages (from Twisted>=21.7.0->scrapy) (21.0.0)\n",
      "Requirement already satisfied: incremental>=24.7.0 in /Users/davidsoteloseguin/Library/Mobile Documents/com~apple~CloudDocs/Personal/Formacion /Bootcamp/Bootcamp KC/Big-Data-Architecture/.venv/lib/python3.13/site-packages (from Twisted>=21.7.0->scrapy) (24.7.2)\n",
      "Requirement already satisfied: typing-extensions>=4.2.0 in /Users/davidsoteloseguin/Library/Mobile Documents/com~apple~CloudDocs/Personal/Formacion /Bootcamp/Bootcamp KC/Big-Data-Architecture/.venv/lib/python3.13/site-packages (from Twisted>=21.7.0->scrapy) (4.12.2)\n",
      "Requirement already satisfied: setuptools in /Users/davidsoteloseguin/Library/Mobile Documents/com~apple~CloudDocs/Personal/Formacion /Bootcamp/Bootcamp KC/Big-Data-Architecture/.venv/lib/python3.13/site-packages (from zope.interface>=5.1.0->scrapy) (75.8.2)\n",
      "Requirement already satisfied: idna in /Users/davidsoteloseguin/Library/Mobile Documents/com~apple~CloudDocs/Personal/Formacion /Bootcamp/Bootcamp KC/Big-Data-Architecture/.venv/lib/python3.13/site-packages (from tldextract->scrapy) (3.10)\n",
      "Requirement already satisfied: requests>=2.1.0 in /Users/davidsoteloseguin/Library/Mobile Documents/com~apple~CloudDocs/Personal/Formacion /Bootcamp/Bootcamp KC/Big-Data-Architecture/.venv/lib/python3.13/site-packages (from tldextract->scrapy) (2.32.3)\n",
      "Requirement already satisfied: requests-file>=1.4 in /Users/davidsoteloseguin/Library/Mobile Documents/com~apple~CloudDocs/Personal/Formacion /Bootcamp/Bootcamp KC/Big-Data-Architecture/.venv/lib/python3.13/site-packages (from tldextract->scrapy) (2.1.0)\n",
      "Requirement already satisfied: filelock>=3.0.8 in /Users/davidsoteloseguin/Library/Mobile Documents/com~apple~CloudDocs/Personal/Formacion /Bootcamp/Bootcamp KC/Big-Data-Architecture/.venv/lib/python3.13/site-packages (from tldextract->scrapy) (3.17.0)\n",
      "Requirement already satisfied: pycparser in /Users/davidsoteloseguin/Library/Mobile Documents/com~apple~CloudDocs/Personal/Formacion /Bootcamp/Bootcamp KC/Big-Data-Architecture/.venv/lib/python3.13/site-packages (from cffi>=1.12->cryptography>=37.0.0->scrapy) (2.22)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/davidsoteloseguin/Library/Mobile Documents/com~apple~CloudDocs/Personal/Formacion /Bootcamp/Bootcamp KC/Big-Data-Architecture/.venv/lib/python3.13/site-packages (from requests>=2.1.0->tldextract->scrapy) (3.4.1)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/davidsoteloseguin/Library/Mobile Documents/com~apple~CloudDocs/Personal/Formacion /Bootcamp/Bootcamp KC/Big-Data-Architecture/.venv/lib/python3.13/site-packages (from requests>=2.1.0->tldextract->scrapy) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/davidsoteloseguin/Library/Mobile Documents/com~apple~CloudDocs/Personal/Formacion /Bootcamp/Bootcamp KC/Big-Data-Architecture/.venv/lib/python3.13/site-packages (from requests>=2.1.0->tldextract->scrapy) (2025.1.31)\n"
     ]
    }
   ],
   "source": [
    "!pip install scrapy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wvTvONB4-rr5"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7rpWuM3f8giS"
   },
   "outputs": [],
   "source": [
    "!ls /content/drive/MyDrive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "919Syw5x_FOi"
   },
   "outputs": [],
   "source": [
    "import scrapy\n",
    "\n",
    "class CWSpider(scrapy.Spider):\n",
    "    name = \"cw\"\n",
    "    start_urls = [\"https://www.creationwatches.com/products/seiko-75/\"]\n",
    "    #file = ficherito\n",
    "    custom_settings = {\n",
    "        'FEED_FORMAT': 'csv',\n",
    "        'FEED_URI': 'watches.csv'\n",
    "    }\n",
    "  # Completemos los comentarios con las partes que queremos extraer\n",
    "\n",
    "    def parse(self, response):\n",
    "        for result in response.css(\"div.catalog_productBox\"):\n",
    "            image = result.css(\"div.imgBox img::attr(src)\").get()\n",
    "            name = result.css(\"a.pro_titel::text\").get()\n",
    "            print(name)\n",
    "            price = result.css(\"div h3 del::text\").get()\n",
    "            print(price)\n",
    "            print('{}, \"{}\", {}'.format(image, name, price).replace(',', ''))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rP2soU0I_s2b"
   },
   "outputs": [],
   "source": [
    "from scrapy.crawler import CrawlerProcess\n",
    "\n",
    "# optional\n",
    "process = CrawlerProcess({\n",
    "    'USER_AGENT': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36'\n",
    "})\n",
    "\n",
    "process.crawl(CWSpider)\n",
    "process.start()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4-M-yt0qHIDo"
   },
   "source": [
    "## Arquitectura de **Scrapy**\n",
    "\n",
    "La arquitectura de Scrapy, es más bien la arquitectura tipica de un scraper, y también de un crawler ya que se pueden hacer ambas cosas a la vez.\n",
    "\n",
    "![Arquitectura_Scrapy](https://doc.scrapy.org/en/0.10.3/_images/scrapy_architecture.png)\n",
    "\n",
    "\n",
    "Dicho esto...\n",
    "\n",
    "# ¿Qué es el **Scrapping**?\n",
    "\n",
    "Comparte las mismas características que el **Crawling** pero que se enfoca más en la **transformación de datos sin estructura** en la web (como el formato HTML) en **datos estructurados** que pueden ser almacenados y analizados en una base de datos, en un csv u otro formato de almacenamiento.\n",
    "\n",
    "## Diferencias entre **Crawling** y **Scrapping**\n",
    "\n",
    "El concepto de web crawling es más genérico, y se refiere a la extracción de los hipervínculos de una web.\n",
    "\n",
    "Normalmente, cuando extraemos datos de una web vamos a tener que parsear esos datos o realizar un parsing para extraer las partes o la información que nos interesa.\n",
    "\n",
    "![diferencias](http://prowebscraping.com/wp-content/uploads/2015/10/web-scraping-vs-web-crawling.png)\n",
    "\n",
    "- Un crawler indexa, descubre y genera fuentes de datos\n",
    "- Un scraper procesa datos con reglas logicas y extrae datos estructurados\n",
    "- Se puede tener un crawler+scraper que funcionen a la vez"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IXIiIDyVt4xq"
   },
   "source": [
    "Generemos archivos a partir de un crawler:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sotceyaCxhQL"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Change the current working directory to /tmp\n",
    "os.chdir(\"/tmp\")\n",
    "\n",
    "# List the files in the /tmp directory\n",
    "print(os.listdir())\n",
    "\n",
    "ficherito = open(\"/tmp/watches.csv\", \"w+\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JDgAXT0CxdEX"
   },
   "outputs": [],
   "source": [
    "import scrapy\n",
    "\n",
    "class CWSpider_2(scrapy.Spider):\n",
    "    name = \"cw\"\n",
    "    start_urls = [\"https://www.creationwatches.com/products/seiko-75/\"]\n",
    "    custom_settings = {\n",
    "        'FEED_FORMAT': 'csv',\n",
    "        'FEED_URI': 'watches.csv'\n",
    "    }\n",
    "\n",
    "    # Completemos los comentarios\n",
    "    def parse(self, response):\n",
    "        for result in response.css(\"div.catalog_productBox\"):\n",
    "            image = result.css(\"div.imgBox img::attr(src)\").get()\n",
    "            if image is None:\n",
    "                name = result.css(\"a.pro_titel::text\").get()\n",
    "                price = result.css(\"div h3 del::text\").get()\n",
    "                yield {\n",
    "                'image': image,\n",
    "                'name': name,\n",
    "                'price': price\n",
    "            }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pTJ9OlqqxhS5"
   },
   "outputs": [],
   "source": [
    "from scrapy.crawler import CrawlerProcess\n",
    "\n",
    "# optional\n",
    "process = CrawlerProcess({\n",
    "    'USER_AGENT': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36'\n",
    "})\n",
    "\n",
    "process.crawl(CWSpider_2)\n",
    "process.start()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MHgHdbEC0HBB"
   },
   "source": [
    "### ¿Por qué falla?\n",
    "\n",
    "**Solo puede haber un CrawlerProcess en el entorno**. Como antes ya iniciamos el proceso, no podemos iniciarlo de nuevo.\n",
    "Debemos **agregar todos los crawl que queramos al proceso y después iniciarlo**.\n",
    "(Reiniciemos el entorno colab para poder hacerlo...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fm3boCVG0ug3"
   },
   "outputs": [],
   "source": [
    "!pip install scrapy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2qz9mHmA0vnf"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TTz7TNgg092t"
   },
   "outputs": [],
   "source": [
    "import scrapy\n",
    "\n",
    "class CWSpider(scrapy.Spider):\n",
    "    name = \"cw\"\n",
    "    start_urls = [\"https://www.creationwatches.com/products/seiko-75/\"]\n",
    "    custom_settings = {\n",
    "        'FEED_FORMAT': 'csv',\n",
    "        'FEED_URI': 'watches.csv'\n",
    "    }\n",
    "\n",
    "    def parse(self, response):\n",
    "        for result in response.css(\"div.catalog_productBox\"):\n",
    "            image = result.css(\"div.imgBox img::attr(src)\").get()\n",
    "            if image is None:\n",
    "                name = result.css(\"a.pro_titel::text\").get()\n",
    "                price = result.css(\"div h3 del::text\").get()\n",
    "                yield {\n",
    "                'image': image,\n",
    "                'name': name,\n",
    "                'price': price\n",
    "            }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VnETQL4W1Kol"
   },
   "outputs": [],
   "source": [
    "class WatchSpider(scrapy.Spider):\n",
    "    name = 'watchspider'\n",
    "    start_urls = [\"https://www.creationwatches.com/products/seiko-75/\"]\n",
    "    custom_settings = {\n",
    "        'FEED_FORMAT': 'json',\n",
    "        'FEED_URI': 'watch_data.json'\n",
    "    }\n",
    "\n",
    "    def parse(self, response):\n",
    "        for result in response.css(\"div.catalog_productBox\"):\n",
    "            image = result.css(\"div.imgBox img::attr(src)\").get()\n",
    "            if image is None:\n",
    "                name = result.css(\"a.pro_titel::text\").get()\n",
    "                price = result.css(\"div h3 del::text\").get()\n",
    "                yield {\n",
    "                'image': image,\n",
    "                'name': name,\n",
    "                'price': price\n",
    "    }\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kJvNCKOQz_Ur"
   },
   "outputs": [],
   "source": [
    "from scrapy.crawler import CrawlerProcess\n",
    "\n",
    "# optional\n",
    "process = CrawlerProcess({\n",
    "    'USER_AGENT': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36'\n",
    "})\n",
    "\n",
    "# Completemos los comentarios...\n",
    "process.crawl(CWSpider)\n",
    "process.crawl(WatchSpider)\n",
    "\n",
    "process.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1K5ErlhJxo4T"
   },
   "outputs": [],
   "source": [
    "!tail /content/watches.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "i58fX_JHIPSS"
   },
   "outputs": [],
   "source": [
    "!tail /content/watch_data.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "q-CPEWa339VU"
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import pprint\n",
    "\n",
    "with open('/content/watch_data.json', 'r') as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "pprint.pprint(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KpYcRK9B5e_v"
   },
   "source": [
    "Existen otras maneras de llamar a Scrapy. Veamos:\n",
    "\n",
    "- [Scrapy common practices](https://doc.scrapy.org/en/latest/topics/practices.html)\n",
    "- [Scrapy shell](https://doc.scrapy.org/en/latest/topics/shell.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pQwFKYko55j5"
   },
   "source": [
    "Existen otras librerías similares a Scrapy, como:\n",
    "- [Beautiful Soup](https://beautiful-soup-4.readthedocs.io/en/latest/)\n",
    "- [Selenium](https://pypi.org/project/selenium/)\n",
    "- [PyQuery](https://pypi.org/project/pyquery/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
